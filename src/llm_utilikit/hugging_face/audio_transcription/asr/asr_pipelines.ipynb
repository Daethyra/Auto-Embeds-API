{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASR with OpenAI/Whisper-large-v3\n",
    "\n",
    "## Build an Automatic Speech Recognition Pipeline\n",
    "\n",
    "- [Whisper (Large) on Hugging Face](https://huggingface.co/openai/whisper-large-v3 \"official openai repository\")\n",
    "- [Hugging Face course on ASR with Whisper (Small)](https://huggingface.co/learn/audio-course/chapter5/asr_models#graduation-to-seq2seq)\n",
    "\n",
    "\n",
    "#### Usage:\n",
    "Whisper large-v3 is supported in Hugging Face ðŸ¤— Transformers through the main branch in the Transformers repo. To run the model, first install the Transformers library through the GitHub repo. For this example, we'll also install ðŸ¤— Datasets to load toy audio dataset from the Hugging Face Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (23.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to c:\\users\\dae\\appdata\\local\\temp\\pip-req-build-bmkp7ik6\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit f4db565b695582891e43a5e042e5d318e28f20b8\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: accelerate in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (0.25.0)\n",
      "Requirement already satisfied: torch in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: datasets[audio] in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from transformers==4.36.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from datasets[audio]) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from datasets[audio]) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from datasets[audio]) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from datasets[audio]) (2.1.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from datasets[audio]) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from datasets[audio]) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets[audio]) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from datasets[audio]) (3.9.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from datasets[audio]) (0.12.1)\n",
      "Requirement already satisfied: librosa in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from datasets[audio]) (0.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from aiohttp->datasets[audio]) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from aiohttp->datasets[audio]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from aiohttp->datasets[audio]) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from aiohttp->datasets[audio]) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from aiohttp->datasets[audio]) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from aiohttp->datasets[audio]) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from requests->transformers==4.36.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from requests->transformers==4.36.0.dev0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from requests->transformers==4.36.0.dev0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from requests->transformers==4.36.0.dev0) (2023.11.17)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from soundfile>=0.12.1->datasets[audio]) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers==4.36.0.dev0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from librosa->datasets[audio]) (3.0.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from librosa->datasets[audio]) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from librosa->datasets[audio]) (1.3.2)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from librosa->datasets[audio]) (1.3.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from librosa->datasets[audio]) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from librosa->datasets[audio]) (0.58.1)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from librosa->datasets[audio]) (1.8.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from librosa->datasets[audio]) (0.3.7)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from librosa->datasets[audio]) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from librosa->datasets[audio]) (1.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from pandas->datasets[audio]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from pandas->datasets[audio]) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from pandas->datasets[audio]) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.41.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from pooch>=1.0->librosa->datasets[audio]) (4.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets[audio]) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dae\\.vscode\\software\\.venv\\lib\\site-packages (from scikit-learn>=0.20.0->librosa->datasets[audio]) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git 'C:\\Users\\dae\\AppData\\Local\\Temp\\pip-req-build-bmkp7ik6'\n"
     ]
    }
   ],
   "source": [
    "# Check that pip is the latest version:\n",
    "%pip install -U pip\n",
    "\n",
    "# Install transformers via source along with other necessary requirements for our notebook\n",
    "%pip install -U git+https://github.com/huggingface/transformers.git accelerate datasets[audio] torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be used with the [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.AutomaticSpeechRecognitionPipeline) class to transcribe audio files of arbitrary length. Transformers uses a chunked algorithm to transcribe long-form audio files, which in-practice is 9x faster than the sequential algorithm proposed by OpenAI (see Table 7 of the [Distil-Whisper paper](https://arxiv.org/abs/2311.00430)). The batch size should be set based on the specifications of your device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dae\\.vscode\\Software\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`AnnotionFormat` is deprecated and will be removed in v4.38. Please use `transformers.image_utils.AnnotationFormat` instead.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check if a GPU is available and set the device accordingly\n",
    "# If a GPU is available, use it (cuda:0); otherwise, use the CPU\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Set the data type for tensors based on the availability of a GPU\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Model ID for the pre-trained model\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "# Load the pre-trained model with specific configurations\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch_dtype,  # Use the previously set data type for tensors\n",
    "    low_cpu_mem_usage=True,  # Optimize memory usage for CPU\n",
    "    use_safetensors=True     # Enable SafeTensors for memory optimization\n",
    ")\n",
    "\n",
    "# Move the model to the specified device (GPU or CPU)\n",
    "model.to(device)\n",
    "\n",
    "# Load the processor for the model\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a pipeline for automatic speech recognition\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,  # Use the loaded model\n",
    "    tokenizer=processor.tokenizer,  # Use the tokenizer from the processor\n",
    "    feature_extractor=processor.feature_extractor,  # Use the feature extractor from the processor\n",
    "    max_new_tokens=128,\n",
    "    chunk_length_s=30,  # Set the chunk length for processing\n",
    "    batch_size=16,  # Set batch size\n",
    "    return_timestamps=True,  # Return timestamps for the transcriptions\n",
    "    torch_dtype=torch_dtype,  # Use the specified data type for tensors\n",
    "    device=device  # Specify the device (GPU or CPU)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset for validation\n",
    "dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sample = dataset[0][\"audio\"]  # Get the first sample from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline on the sample and get the result\n",
    "result = pipe(sample)\n",
    "\n",
    "# Print the recognized text from the audio\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transcribe a local audio file, simply pass the path to your audio file when you call the pipeline:\n",
    "\n",
    "```python\n",
    "- result = pipe(sample)\n",
    "+ result = pipe(\"audio.mp3\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Target Language\n",
    "Whisper predicts the language of the source audio automatically. If the source audio language is known *a-priori*, it can be passed as an argument to the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe(sample, generate_kwargs={\"language\": \"english\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Task\n",
    "By default, Whisper performs the task of speech transcription, where the source audio language is the same as the target text language. To perform speech translation, where the target text is in English, set the task to \"translate\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe(sample, generate_kwargs={\"task\": \"translate\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timestamps\n",
    "Finally, the model can be made to predict timestamps.\n",
    "\n",
    "For ***sentence-level timestamps***, pass the `return_timestamps` argument with `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe(sample, return_timestamps=True)\n",
    "print(result[\"chunks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***And for word-level timestamps, pass `\"Word\"`***:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe(sample, return_timestamps=\"word\")\n",
    "print(result[\"chunks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above arguments can be used in ***isolation*** or in ***combination***. \n",
    "\n",
    "For example, to perform the task of speech transcription where the source audio is in French, and we want to return sentence-level timestamps, the following can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipe(sample, return_timestamps=True, generate_kwargs={\"language\": \"french\", \"task\": \"translate\"})\n",
    "print(result[\"chunks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Speed & Memory Improvements\n",
    "\n",
    "#### Flash Attention\n",
    "We recommend using [Flash-Attention 2](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#flashattention-2) if your GPU allows for it. To do so, you first need to install [Flash Attention](https://github.com/Dao-AILab/flash-attention):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flash-attn -no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then all you have to do is to pass `use_flash_attention_2=True` to *`from_pretrained`*:\n",
    "\n",
    "```python\n",
    "- model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, \n",
    "    low_cpu_mem_usage=True, use_safetensors=True)\n",
    "\n",
    "+ model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, \n",
    "    low_cpu_mem_usage=True, use_safetensors=True, use_flash_attention_2=True) # Use Flash Attention 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a demo with Gradio\n",
    "\n",
    "Weâ€™ll define a function that takes the filepath for an audio input and passes it through the pipeline. \n",
    "\n",
    "Here, the pipeline automatically takes care of loading the audio file, resampling it to the correct sampling rate, and running inference with the model. We can then simply return the transcribed text as the output of the function. \n",
    "\n",
    "To ensure our model can handle audio inputs of arbitrary length, weâ€™ll enable chunking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_speech(filepath):\n",
    "    result = pipe(\n",
    "        filepath,\n",
    "        max_new_tokens=128,\n",
    "        generate_kwargs={\n",
    "            \"task\": \"transcribe\",\n",
    "            \"language\": \"english\",\n",
    "        },\n",
    "        chunk_length_s=30,\n",
    "        batch_size=8,\n",
    "    )\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weâ€™ll use the Gradio blocks feature to launch two tabs on our demo: one for microphone transcription, and the other for file upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Blocks()\n",
    "\n",
    "mic_transcribe = gr.Interface(\n",
    "    fn=transcribe_speech,\n",
    "    inputs=gr.Audio(sources=\"microphone\", type=\"filepath\"),\n",
    "    outputs=gr.outputs.Textbox(),\n",
    ")\n",
    "\n",
    "file_transcribe = gr.Interface(\n",
    "    fn=transcribe_speech,\n",
    "    inputs=gr.Audio(sources=\"upload\", type=\"filepath\"),\n",
    "    outputs=gr.outputs.Textbox(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch the Gradio demo using our two blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with demo:\n",
    "    gr.TabbedInterface(\n",
    "        [mic_transcribe, file_transcribe],\n",
    "        [\"Transcribe Microphone\", \"Transcribe Audio File\"],\n",
    "    )\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
