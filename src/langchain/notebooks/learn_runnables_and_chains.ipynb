{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation using Runnables and Chains w/ LangChain\n",
    "Enhance generation with specialized knowledge.\n",
    "\n",
    "**Purpose**:\n",
    "This notebook's purpose is to teach you how to build your own custom `Runnable`s from the `LangChain` ecosystem to build your own RAG app.\n",
    "\n",
    "## Definitions: `Runnables` and `Chains`\n",
    "\n",
    "*Runnables*:\n",
    "\n",
    "• A Runnable represents a unit of work that can be executed.\n",
    "\n",
    "• It can perform a specific task or action, such as making an API call, processing data, or running a machine learning model.\n",
    "\n",
    "• Runnables can have input and output types specified, and they can be composed together to form more complex workflows.\n",
    "\n",
    "• They are designed to be flexible and reusable components that can be easily combined and configured.\n",
    "\n",
    "• Examples of Runnables include API calls, data processing functions, and machine learning models.\n",
    "\n",
    "*Chains*:\n",
    "\n",
    "• A Chain is a sequence of Runnables that are executed in a specific order.\n",
    "\n",
    "• Chains provide a way to string together multiple Runnables to create a workflow or pipeline.\n",
    "\n",
    "• Each Runnable in the Chain takes the output of the previous Runnable as its input.\n",
    "\n",
    "• Chains can be used to build complex applications by combining and orchestrating the execution of multiple Runnables.\n",
    "\n",
    "• They provide a higher-level abstraction for organizing and structuring the flow of data and operations.\n",
    "\n",
    "• Examples of Chains include data processing pipelines, machine learning workflows, and API request/response sequences.\n",
    "\n",
    "---\n",
    "\n",
    "### **Deeper explanation**:\n",
    "\n",
    "In the process of building an AI chatbot, we often need to connect different components together to create a functional system.\n",
    "\n",
    "One way to achieve this is by *chaining* these components, ensuring that the output of one component is properly passed to the next component for further processing. To accomplish this, we can directly call the functions or methods of each component and pass the output as arguments to the next component. \n",
    "\n",
    "- This straightforward approach works well when we only need to pass the output from one component to another ***without any*** additional processing or transformations in between.\n",
    "\n",
    "However, in more complex scenarios where we require intermediate processing or transformations on the output, we can use a concept called \"runnables.\" Runnables provide a flexible and modular way to encapsulate and compose these processing steps within a chain.\n",
    "\n",
    "By using runnables, we can easily add additional functionality, such as filtering or modifying the output, before passing it to the next component. \n",
    "\n",
    "- This allows us to *customize the behavior* of the chatbot and *ensure* that the output is properly prepared for the subsequent steps.\n",
    "\n",
    "#### \"How are `Runnable`s different than normal classes?\"\n",
    "\n",
    "*Similarities*:\n",
    "\n",
    "• Runnables can have methods and attributes, just like normal classes.\n",
    "\n",
    "• They can define and implement their own logic and functionality.\n",
    "\n",
    "• Runnables can have constructor arguments and can be instantiated with different configurations.\n",
    "\n",
    "• They can be subclassed and inherit from other runnables or normal classes.\n",
    "\n",
    "• Runnables can have static and class methods, allowing for shared functionality across instances.\n",
    "\n",
    "*Differences*:\n",
    "\n",
    "• Runnables are designed to be executed as part of a larger system or workflow, often in a distributed or parallelized manner.\n",
    "\n",
    "• They are typically used for data processing, transformation, or analysis tasks.\n",
    "• Runnables have specific interfaces and methods that define how they interact with other runnables and the overall system.\n",
    "\n",
    "• They can be composed and combined with other runnables to create complex workflows.\n",
    "\n",
    "• Runnables often have additional features and capabilities specific to the Langchain platform, such as input and output type validation, configuration management, and error handling.\n",
    "\n",
    "• They can be executed asynchronously and in parallel, taking advantage of distributed computing resources.\n",
    "\n",
    "• Runnables can be versioned and deployed as part of a larger system, allowing for easy updates and maintenance.\n",
    "\n",
    "#### \"How do I decide to use either a `Runnable` or a `Chain`?\"\n",
    "Ultimately, the decision to use runnables or a more straightforward sequential approach depends on the specific requirements and complexity of the chatbot system. You might find yourself using one, both, or neither based on your needs.\n",
    "\n",
    "In summary, \n",
    "1. Chains, which are sequences of interconnected tasks, can operate effectively on their own, without the need for Runnables. They are designed to link various components of a system in a specific order, allowing for the smooth execution of a workflow or pipeline. This makes them particularly useful in scenarios where a straightforward, sequential process is sufficient and where the complexity of Runnables is not required.\n",
    "\n",
    "2. Runnables resemble traditional classes but offer enhanced functionality, particularly in complex AI chatbot systems. They facilitate the integration and processing of outputs between different components, allowing for customization and increased flexibility in system design. This makes Runnables ideal for scenarios requiring more than just sequential processing, such as when intermediate steps or specific transformations of data are necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq openai tiktoken chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API Key Directly\n",
    "import os\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# Load from an .env file\n",
    "# import dotenv\n",
    "\n",
    "# dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Chunks with `RecursiveCharacterTextSplitter`\n",
    "\n",
    "To make effective use of our loaded documents (files) we need to split them into manageable chunks.\n",
    "\n",
    "Generally speaking, smaller chunks warrant more accurate results, but may take longer to process.\n",
    "\n",
    "### Go Deeper\n",
    "\n",
    "#### Accuracy with Smaller Chunks\n",
    "* **Increased Focus**: Smaller chunks of text or queries allow the system to focus on a more specific set of information. This specificity can lead to more accurate and relevant results because the system is not overwhelmed by too much or too broad information.\n",
    "* **Contextual Relevance**: With a narrower focus, the likelihood of retrieving information that is contextually relevant to more specific queries, enhancing the accuracy of the response.\n",
    "\n",
    "#### Processing Time\n",
    "* **Multiple Queries**: Smaller chunks might require multiple queries to cover a topic comprehensively. Each query involves a separate retrieval process, which cumulatively can take more time.\n",
    "* **Trade-off Between Depth and Breadth**: While smaller queries allow for a depth in a specific area, they might necessitate multiple rounds of retrieval to get a broad understanding, thus increasing overall processing time.\n",
    "\n",
    "#### System Limitations and Efficiency:\n",
    "* **Computational Load**: Smaller chunks means more frequent calls to the retrieval system. Depending on the efficiency of the system, this can either slow down the process due to computational load or, if the system is highly efficient, might not significantly impact the processing time.\n",
    "\n",
    "The following cell demonstrates how to split the loaded data into chunks using the Langchain library. We'll instantiate a variable, `text_splitter`, with the `RecursiveCharacterTextSplitter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Directory of Files with `PyPDFDirectoryLoader`\n",
    "\n",
    "Now that we have a text splitter, we can use it to split our documents into chunks.\n",
    "We'll accomplish this by using the `PyPDFDirectoryLoader` class.\n",
    "\n",
    "Start by using the `PyPDFDirectoryLoader` class to load the data by directly passing in the path to the directory.\n",
    "\n",
    "For this example, we'll use `./data` and split it into chunks using the `text_splitter` variable.\n",
    "\n",
    "Be sure to create this subdirectory in this notebook's current working directory and place PDF files you'd like to interrogate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# Create loader\n",
    "pydir_loader = PyPDFDirectoryLoader(\"./data\")\n",
    "\n",
    "# Load data\n",
    "docs = pydir_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into chunks\n",
    "split_docs = [text_splitter.split_text(doc.page_content) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KENN\n",
      "Befo\n",
      "“The\n",
      "1. P\n",
      "of p\n",
      "Now \n",
      "Howe\n",
      "I’d \n",
      "Impo\n",
      "Lear\n",
      "Beca\n",
      "Not \n",
      "ment\n",
      "good\n",
      "Node\n",
      "stro\n",
      "3. L\n",
      "free\n",
      "Whil\n",
      "lear\n",
      "Stud\n",
      "Whil\n",
      "from\n",
      "4. P\n",
      "Anot\n",
      "Now \n",
      "they\n",
      "the \n",
      "Deve\n",
      "Git,\n",
      "http\n",
      "5. N\n",
      "codi\n",
      "fit.\n",
      "Atte\n",
      "comp\n",
      "me m\n",
      "If y\n",
      "appr\n",
      "The \n",
      "Alth\n",
      "star\n",
      "WEEK\n",
      "This\n"
     ]
    }
   ],
   "source": [
    "# Print first 4 chunks\n",
    "for doc_chunks in split_docs:\n",
    "    for chunk in doc_chunks:\n",
    "        print(chunk[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve Embeddings from OpenAI\n",
    "\n",
    "This cell defines a `Runnable`, `retrieve_embeddings` that retrieves embeddings for a list of texts using the `OpenAIEmbeddings` class. The function takes a list of texts as input and returns a list of embeddings, one for each text.\n",
    "\n",
    "To use this function, pass a list of texts to the `retrieve_embeddings` function, and it will return a list of embeddings, one for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "class RetrieveEmbeddingsRunnable(Runnable):\n",
    "    lc_attributes = {\n",
    "        \"input\": str,\n",
    "    }\n",
    "    def invoke(self, input, config=None):\n",
    "        try:\n",
    "            embeddings_model = OpenAIEmbeddings()\n",
    "            embeddings = embeddings_model.embed_documents(input)\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving embeddings: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Embeddings\n",
    "The following code defines the class, which retrieves embeddings from a directory of files and uses Chroma as the vector store and retriever. The `retrieve_embeddings` method performs the retrieval process, invoking the necessary Runnables and storing the embeddings in Chroma. The class provides methods to access the vector store and retriever objects for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import VectorStoreRetriever\n",
    "\n",
    "class EmbeddingsRetrieval(Runnable):\n",
    "    def __init__(self, directory_path: str, chunk_size: int, overlap_ratio: float):\n",
    "        self.directory_path = directory_path\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.vector_store = None\n",
    "        self.retriever = None\n",
    "\n",
    "    def retrieve_embeddings(self):\n",
    "        try:\n",
    "            # Create the Runnables\n",
    "            directory_loader = DirectoryLoaderRunnable(directory_path=self.directory_path)\n",
    "            data_splitter = SplitDataIntoChunksRunnable(chunk_size=self.chunk_size, overlap_ratio=self.overlap_ratio)\n",
    "            embeddings_retriever = RetrieveEmbeddingsRunnable()\n",
    "\n",
    "            # Invoke the Runnables\n",
    "            processed_files = directory_loader.invoke()\n",
    "            chunks = data_splitter.invoke(processed_files)\n",
    "            embeddings = embeddings_retriever.invoke(chunks)\n",
    "\n",
    "            # Use Chroma as the vector store\n",
    "            self.vector_store = Chroma()\n",
    "            self.vector_store.add_documents(embeddings)\n",
    "\n",
    "            # Use Chroma as the retriever\n",
    "            self.retriever = self.vector_store.as_retriever()\n",
    "\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving embeddings: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of EmbeddingsRetrieval\n",
    "retrieval = EmbeddingsRetrieval(directory_path=\"data\", chunk_size=1000, overlap_ratio=0.2)\n",
    "\n",
    "# Retrieve the embeddings\n",
    "embeddings = retrieval.retrieve_embeddings()\n",
    "\n",
    "# Use the embeddings as needed\n",
    "# For a quick check, print the embeddings\n",
    "for embedding in embeddings:\n",
    "    print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of ChatOpenAI Chatbot Chain Examples\n",
    "These end-to-end examples were generated at https://chat.langchain.com/ on 12/22/23:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationWindowBufferMemory\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import SelfQueryRetriever\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.parsers import StrOutputParser\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# Create the chat model\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "# Create the prompt template with memory placeholders\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the memory with a window size of 2\n",
    "memory = ConversationWindowBufferMemory(window_size=2, return_messages=True)\n",
    "\n",
    "# Create the Chroma vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = Chroma.from_documents([], embeddings)\n",
    "\n",
    "# Create the Chroma retriever\n",
    "retriever = SelfQueryRetriever(vector_store)\n",
    "\n",
    "# Create the chain\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history=RunnableLambda(memory.load_memory_variables) | lambda x: x.get(\"history\", [])\n",
    "    )\n",
    "    | prompt_template\n",
    "    | chat_model\n",
    "    | retriever\n",
    ")\n",
    "\n",
    "# Create the output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Define the user input\n",
    "user_input = \"Hi, how can I help you?\"\n",
    "\n",
    "# Invoke the chain\n",
    "output = chain.invoke({\"input\": user_input})\n",
    "\n",
    "# Parse the output\n",
    "parsed_output = output_parser.parse(output)\n",
    "\n",
    "# Update the memory with the user input and model output\n",
    "memory.save_context({\"input\": user_input}, {\"output\": parsed_output})\n",
    "\n",
    "# Print the parsed output\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ConversationWindowBufferMemory' from 'langchain.memory' (c:\\Users\\dae\\.vscode\\Software\\.venv\\lib\\site-packages\\langchain\\memory\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationChain\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ConversationWindowBufferMemory\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparsers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StrOutputParser\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ConversationWindowBufferMemory' from 'langchain.memory' (c:\\Users\\dae\\.vscode\\Software\\.venv\\lib\\site-packages\\langchain\\memory\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationWindowBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.parsers import StrOutputParser\n",
    "from langchain.vector_stores import Chroma\n",
    "\n",
    "# Create the chat model\n",
    "chat_model = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.25\n",
    "    )\n",
    "\n",
    "# Create the memory\n",
    "memory = ConversationWindowBufferMemory(window_size=5)\n",
    "\n",
    "# Create the vector store and retriever\n",
    "vector_store = Chroma()\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Create the chatbot chain\n",
    "chatbot_chain = ConversationChain(\n",
    "    llm=chat_model,\n",
    "    memory=memory,\n",
    "    retriever=retriever,\n",
    "    parser=StrOutputParser(),\n",
    ")\n",
    "\n",
    "# Run the chatbot chain\n",
    "response = chatbot_chain.invoke(\"Hello!\")\n",
    "print(response[\"response\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
