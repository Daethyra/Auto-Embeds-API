{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation using Runnables and Chains w/ LangChain\n",
    "\n",
    "Enhance generation with specialized knowledge.\n",
    "\n",
    "**Purpose**:\n",
    "This notebook's purpose is to teach you how to build your own custom `Runnable`s from the `LangChain` ecosystem to build your own RAG app.\n",
    "\n",
    "## Definitions: `Runnables` and `Chains`\n",
    "\n",
    "### *Runnables*:\n",
    "\n",
    "• A Runnable represents a unit of work that can be executed.\n",
    "\n",
    "• It can perform a specific task or action, such as making an API call, processing data, or running a machine learning model.\n",
    "\n",
    "• Runnables can have input and output types specified, and they can be composed together to form more complex workflows.\n",
    "\n",
    "• They are designed to be flexible and reusable components that can be easily combined and configured\n",
    "\n",
    "• Require an `invoke` method, which is used to execute the Runnable.\n",
    "\n",
    "• Examples of Runnables include API calls, data processing functions, and machine learning models.\n",
    "\n",
    "### *Chains*:\n",
    "\n",
    "• A Chain is a sequence of Runnables that are executed in a specific order.\n",
    "\n",
    "• Chains provide a way to string together multiple Runnables to create a workflow or pipeline.\n",
    "\n",
    "• Each Runnable in the Chain takes the output of the previous Runnable as its input.\n",
    "\n",
    "• Chains can be used to build complex applications by combining and orchestrating the execution of multiple Runnables.\n",
    "\n",
    "• They provide a higher-level abstraction for organizing and structuring the flow of data and operations.\n",
    "\n",
    "• Examples of Chains include data processing pipelines, machine learning workflows, and API request/response sequences.\n",
    "\n",
    "## **Deeper explanation**:\n",
    "\n",
    "In the process of building an AI chatbot, we often need to connect different components together to create a functional system.\n",
    "\n",
    "One way to achieve this is by *chaining* these components, ensuring that the output of one component is properly passed to the next component for further processing. To accomplish this, we can directly call the functions or methods of each component and pass the output as arguments to the next component. \n",
    "\n",
    "- This straightforward approach works well when we only need to pass the output from one component to another ***without any*** additional processing or transformations in between.\n",
    "\n",
    "However, in more complex scenarios where we require intermediate processing or transformations on the output, we can use a concept called \"runnables.\" Runnables provide a flexible and modular way to encapsulate and compose these processing steps within a chain.\n",
    "\n",
    "By using runnables, we can easily add additional functionality, such as filtering or modifying the output, before passing it to the next component. \n",
    "\n",
    "- This allows us to *customize the behavior* of the chatbot and *ensure* that the output is properly prepared for the subsequent steps.\n",
    "\n",
    "### \"How are `Runnable`s different than normal classes?\"\n",
    "\n",
    "*Similarities*:\n",
    "\n",
    "• Runnables can have methods and attributes, just like normal classes.\n",
    "\n",
    "• They can define and implement their own logic and functionality.\n",
    "\n",
    "• Runnables can have constructor arguments and can be instantiated with different configurations.\n",
    "\n",
    "*Differences*:\n",
    "\n",
    "• Runnables are designed to be executed as part of a larger system or workflow, often in a distributed or parallelized manner.\n",
    "\n",
    "• They are typically used for data processing, transformation, or analysis tasks.\n",
    "• Runnables have specific interfaces and methods that define how they interact with other runnables and the overall system.\n",
    "\n",
    "• They can be composed and combined with other runnables to create complex workflows.\n",
    "\n",
    "• Runnables often have additional features and capabilities specific to the Langchain platform, such as input and output type validation, configuration management, and error handling.\n",
    "\n",
    "• They can be executed asynchronously and in parallel, taking advantage of distributed computing resources.\n",
    "\n",
    "• Runnables can be versioned and deployed as part of a larger system, allowing for easy updates and maintenance.\n",
    "\n",
    "### \"How do I decide to use either a `Runnable` or a `Chain`?\"\n",
    "Ultimately, the decision to use runnables or a more straightforward sequential approach depends on the specific requirements and complexity of the chatbot system. You might find yourself using one, both, or neither based on your needs.\n",
    "\n",
    "In summary, \n",
    "1. Chains, which are sequences of interconnected tasks, can operate effectively on their own, without the need for Runnables. They are designed to link various components of a system in a specific order, allowing for the smooth execution of a workflow or pipeline. This makes them particularly useful in scenarios where a straightforward, sequential process is sufficient and where the complexity of Runnables is not required.\n",
    "\n",
    "2. Runnables resemble traditional classes but offer enhanced functionality, particularly in complex AI chatbot systems. They facilitate the integration and processing of outputs between different components, allowing for customization and increased flexibility in system design. This makes Runnables ideal for scenarios requiring more than just sequential processing, such as when intermediate steps or specific transformations of data are necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq openai tiktoken chromadb langchain langchain-openai faiss-cpu beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API Key Directly\n",
    "import os\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "# Load from an .env file\n",
    "# import dotenv\n",
    "\n",
    "# dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Executing a Custom Runnable\n",
    "\n",
    "In this section, we will put into practice the concepts of `Runnables` that we've discussed earlier. We will create a custom `Runnable` called `AddNumbersRunnable` and demonstrate how to execute it within our application.\n",
    "\n",
    "### Step 1: Define the Runnable\n",
    "\n",
    "First, we instantiate our `AddNumbersRunnable`. This is a class that we've designed to perform a specific task—in this case, adding two numbers. The design of this class follows the principles of `Runnables` in the LangChain ecosystem, making it a modular and reusable component.\n",
    "\n",
    "### Step 2: Prepare the Input\n",
    "\n",
    "Next, we prepare the input for our runnable using the `InputType` class. This class is a Pydantic model that ensures our input data is structured and typed correctly. By creating an instance of `InputType`, we are packaging our data (the two numbers we want to add) in a way that our `Runnable` can easily understand and process.\n",
    "\n",
    "### Step 3: Execute the Runnable\n",
    "\n",
    "With our input ready, we call the `run` method of our `AddNumbersRunnable` instance. This method encapsulates the logic of our task and is responsible for executing the work defined by the `Runnable`. It takes our structured input, performs the addition, and returns an output in the form of an `OutputType` instance.\n",
    "\n",
    "### Step 4: Display the Result\n",
    "\n",
    "Finally, we display the result of our runnable's execution. The `OutputType` class defines the expected structure of the output from our `Runnable`. By accessing the `result` attribute, we can retrieve the sum of the two numbers and present it to the user.\n",
    "\n",
    "This simple example illustrates the power of `Runnables` and how they can be used to build clean, organized, and reusable components within your applications. By following this pattern, you can create more complex workflows and applications using the LangChain library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.base import Runnable\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class AddNumbersRunnable(Runnable):\n",
    "    \"\"\"\n",
    "    A class representing a runnable that adds two numbers together.\n",
    "\n",
    "    Attributes:\n",
    "        InputType (BaseModel): A Pydantic model representing the input to the runnable. It has two attributes: num1 and num2, both integers.\n",
    "        OutputType (BaseModel): A Pydantic model representing the output of the runnable. It has one attribute: result, an integer.\n",
    "\n",
    "    Methods:\n",
    "        run(input: InputType) -> OutputType: This method takes an instance of InputType as input, adds the two numbers together, and returns an instance of OutputType containing the sum.\n",
    "        invoke(input: InputType) -> OutputType: This method is a wrapper around the run method. It takes an instance of InputType as input and returns the result of calling the run method with the same input.\n",
    "    \"\"\"\n",
    "    class InputType(BaseModel):\n",
    "        num1: int\n",
    "        num2: int\n",
    "\n",
    "    class OutputType(BaseModel):\n",
    "        result: int\n",
    "\n",
    "    def run(self, input: InputType) -> OutputType:\n",
    "        result = input.num1 + input.num2\n",
    "        output = self.OutputType(result=result)\n",
    "        return output\n",
    "    \n",
    "    def invoke(self, input: InputType) -> OutputType:\n",
    "        return self.run(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of adding 5 and 3 is: 8\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the AddNumbersRunnable class.\n",
    "# This class is an example of a Runnable, which is a core concept in LangChain for creating modular and reusable units of work.\n",
    "# As we've learned, Runnables like this one encapsulate specific tasks—in this case, adding two numbers.\n",
    "add_numbers = AddNumbersRunnable()\n",
    "\n",
    "# Prepare the input data for the runnable.\n",
    "# The InputType class is a Pydantic model that defines the expected structure of the input, ensuring type safety and validation.\n",
    "# Here, we're creating an instance of InputType with two numbers, demonstrating how inputs are structured for Runnables.\n",
    "input_data = AddNumbersRunnable.InputType(num1=5, num2=3)\n",
    "\n",
    "# Execute the runnable with the provided input data.\n",
    "# The run method is where the logic of the Runnable is executed. This method is a clear example of how a Runnable performs its task.\n",
    "# By calling this method, we're following the pattern of Runnables where they take an input, process it, and produce an output.\n",
    "output_data = add_numbers.run(input_data)\n",
    "\n",
    "# Display the result of the runnable's execution.\n",
    "# The OutputType class defines the structure of the output, which is another aspect of Runnables that promotes consistency and predictability.\n",
    "# This print statement not only shows the result but also reinforces the concept of Runnables having defined inputs and outputs.\n",
    "print(f\"The result of adding {input_data.num1} and {input_data.num2} is: {output_data.result}\")  # Output: 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of ChatOpenAI Chatbot Chain Examples\n",
    "These end-to-end examples were generated at https://chat.langchain.com/ on 12/22/23:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the chat model\n",
    "chat_model = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.25,   \n",
    "    )\n",
    "\n",
    "# Create the prompt template with memory placeholders\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the memory with a window size of 2\n",
    "memory = ConversationBufferWindowMemory(k=2, return_messages=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# Initialize a loader, and load documents. \\\n",
    "    # We'll use the WebBaseLoader to load documents from the web.\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/integrations/document_loaders/web_base\")\n",
    "\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=256, chunk_overlap=25)\n",
    "\n",
    "# Split our documents into chunks using the CharacterTextSplitter\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Initialize the embeddings model\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=\"\")\n",
    "\n",
    "# Initialize a FAISS index with our embedded documents\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output parser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Create the chain\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        history = RunnableLambda(memory.load_memory_variables) | (lambda x: x.get(\"history\", []))\n",
    "    )\n",
    "    | prompt_template\n",
    "    | chat_model\n",
    "    | retriever\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "# Define the user input\n",
    "user_input = \"Hi, how can I help you?\"\n",
    "\n",
    "# Invoke the chain\n",
    "output = chain.invoke({\"input\": user_input})\n",
    "\n",
    "# Parse the output\n",
    "parsed_output = output_parser.parse(output)\n",
    "\n",
    "# Update the memory with the user input and model output\n",
    "memory.save_context({\"input\": user_input}, {\"output\": parsed_output})\n",
    "\n",
    "# Print the parsed output\n",
    "print(parsed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1260384661.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 27\u001b[1;36m\u001b[0m\n\u001b[1;33m    chain =\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.conversation.base import ConversationChain\n",
    "from langchain.memory.buffer_window import ConversationBufferWindowMemory\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Create the chat model\n",
    "chat_model = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo-1106\",\n",
    "    temperature=0.25,\n",
    "    )\n",
    "\n",
    "# Create the memory\n",
    "memory = ConversationBufferWindowMemory(k=5)\n",
    "\n",
    "# Create the vector store and retriever\n",
    "vector_store = Chroma()\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Create the chatbot chain\n",
    "chatbot_chain = ConversationChain(\n",
    "    llm=chat_model,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "chain = \n",
    "\n",
    "# Run the chatbot chain\n",
    "response = chatbot_chain.invoke(\"Hello!\")\n",
    "print(response[\"response\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
