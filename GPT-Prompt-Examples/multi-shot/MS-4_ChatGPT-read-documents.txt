[Assignment 1]:
- AI web researcher, documentation reader, programming guide
- *NO* prose
- Professionally developed grade code

[Task 1]:
"Read the following."
- I'm Daethyra, I'm building an automated Threader that posts updates to Threads when an event happens on a Github repository and a webhook is sent; and that is based on the Threads-py project. Build an entirely new module that does everything I need and more:

- https://raw.githubusercontent.com/junhoyeo/threads-py/main/threadspy/threads_api.py

- No Prose
- No Output
-- Unless Requested

[Notes]:
"""
from threadspy import ThreadsAPI
import os
import dotenv
--- required^

* Looking for enhanced security via python-dotenv
* Looking for LangChain implementation to automatically post Threads on configurable events
* Retry mechanisms are required
* End goal: create pluggable project that automatically posts Threads based on user-defined event occurences
"""

[Task 2]:
"Read each link one by one and learn how to thoroughly implement and leverage the power and capabilities of LangChain. Before outputting anything to the user, ask for permission. Internet searches, link clicking, and navigation do not require user-granted permission."

-https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.chains
-https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.chat_models
-https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.docstore
-https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.document_loaders
-https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.document_transformers
-https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.env
-https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.embeddings
-https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.llms
-https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.memory
-https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.requests
-https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.retrievers
-https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.sql_database
-https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.tools
-https://api.python.langchain.com/en/latest/api_reference.html#module-langchain.utils
---


[Assignment 2]:
- AI Programming assistant
- Minimal prose
- Professionally developed grade code

[Task]:
- Step 1 -
- Analyze the provided code, brainstorm how to takeover as much functionality with LangChain as possible based on what you learned from web-searching

- Step 2 -
- Upgrade the module 3 times over in your head that includes functionality of asynchronously waiting for user input. On input, activate the event, on the event, push to Threads. Github webhook handler and listener. Use the `transformers` python package to load Llama2 from Hugging Face.

[Notes]:
"""
-from datasets import load_dataset
dataset = load_dataset("fka/awesome-chatgpt-prompts")
from transformers import LlamaForCausalLM, LlamaTokenizer
tokenizer = LlamaTokenizer.from_pretrained("/output/path")
model = LlamaForCausalLM.from_pretrained("/output/path")

-pip install gpt4all
 pip install chromadb

-from langchain.document_loaders import WebBaseLoader
loader = WebBaseLoader("https://lilianweng.github.io/posts/2023-06-23-agent/")
data = loader.load()
from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
all_splits = text_splitter.split_documents(data)

-from langchain.vectorstores import Chroma
from langchain.embeddings import GPT4AllEmbeddings
vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())

-question = "What are the approaches to Task Decomposition?"
docs = vectorstore.similarity_search(question)
len(docs)
"""

- Step 3 -
- Provide all necessary modules for the project to be fully functional and testable
- Modules must be pluggable by AutoGPT
-- Read the summary 
"""
Auto-GPT is an experimental, open-source AI agent that uses the GPT-4 language model. It's an "AI agent" that can be given a goal in natural language and will attempt to achieve it by breaking it into sub-tasks and using the internet and other tools in an automatic loop. Auto-GPT can chain together tasks to achieve a big-picture goal set by the user. It can also use keywords to generate code snippets or entire programs based on specifications.
Auto-GPT uses OpenAI's GPT-4 or GPT-3.5 APIs, and is among the first examples of an application using GPT-4 to perform autonomous tasks. It's considered a crucial example to mention when discussing AI agents and their potential.
Auto-GPT can work on its own and can make its own decisions, while AgentGPT needs human intervention to operate. Auto-GPT can create its own prompts while AgentGPT depends on user inputs.
"""

- Step 4 -
- Output all modules to code blocks as programmatically logically optimally as possible
- Include and integrate all proposed upgrades and improvements
-- Ensure you add automatic Thread posting in response to on user-defined events
---


[INSTRUCTIONS]:
"Let's do things step by step so we make sure we have the right answer before moving on to the next one."
- Assignment-->Task--Steps--Task[...]-->Assignment[...]
- Wait to output the module, and any other information, until after you've done all of your research. The final step is to brainstorm 3 solutions, review them, merge them, and finalize one based on *all* of the user's needs.
- Create a Python application that listens for and handles Github webhooks and automatically posts the commit message, with a link to the update, and a summary of the update, in a Thread via Threads-py packages regarding the push
---
